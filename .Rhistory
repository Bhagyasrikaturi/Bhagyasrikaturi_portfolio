ggplot(Walmart, aes(x = Store, y = Weekly_Sales)) +
geom_boxplot(fill = "#FF4081", alpha = 0.7) +
labs(title = "Boxplot of Weekly Sales by Store",
x = "Store", y = "Weekly Sales") +
theme_minimal()
# Scatter plot for Weekly Sales and Temperature
ggplot(Walmart, aes(x = Temperature, y = Weekly_Sales)) +
geom_point(color = "#1976D2", alpha = 0.7) +
labs(title = "Scatter Plot of Weekly Sales vs Temperature",
x = "Temperature", y = "Weekly Sales") +
theme_minimal()
# Quartile plot
quartile_data <- Walmart %>%
group_by(Date) %>%
summarise(Q1 = quantile(Weekly_Sales, 0.25),
Q3 = quantile(Weekly_Sales, 0.75))
ggplot(quartile_data, aes(x = Date)) +
geom_line(aes(y = Q1), color = "#673AB7", linetype = "dashed") +
geom_line(aes(y = Q3), color = "#673AB7", linetype = "dashed") +
labs(title = "Quartile Sales Over Time",
x = "Date", y = "Weekly Sales",
color = "Quartile") +
theme_minimal()
# Model Evaluation
# Split the data into training and testing sets
set.seed(123)
train_indices <- createDataPartition(Walmart$Weekly_Sales, p = 0.7, list = FALSE)
trainData <- Walmart[train_indices, ]
testData <- Walmart[-train_indices, ]
# Evaluate the linear regression model
lm_model <- lm(Weekly_Sales ~ Temperature + Fuel_Price + CPI + Unemployment + Holiday_Flag, data = trainData)
lm_predictions <- predict(lm_model, newdata = testData)
lm_r2 <- cor(lm_predictions, testData$Weekly_Sales)^2
lm_rmse <- sqrt(mean((lm_predictions - testData$Weekly_Sales)^2))
# Evaluate the random forest model
rf_model <- randomForest(Weekly_Sales ~ Temperature + Fuel_Price + CPI + Unemployment + Holiday_Flag, data = trainData)
rf_predictions <- predict(rf_model, newdata = testData)
rf_r2 <- cor(rf_predictions, testData$Weekly_Sales)^2
rf_rmse <- sqrt(mean((rf_predictions - testData$Weekly_Sales)^2))
cat("Linear Regression Model:\n")
cat("R-squared:", lm_r2, "\n")
cat("RMSE:", lm_rmse, "\n\n")
cat("Random Forest Model:\n")
cat("R-squared:", rf_r2, "\n")
cat("RMSE:", rf_rmse, "\n")
library(dplyr)
library(tidyr)
library(ggplot2)
library(corrplot)
library(lubridate)
library(kableExtra)
# Read CSV file
NFLX <- read.csv("C:/Users/HP/Downloads/NFLX.csv", header=TRUE)
names(NFLX)
library(dplyr)
library(tidyr)
library(ggplot2)
library(corrplot)
library(lubridate)
library(kableExtra)
# Read CSV file
NFLX <- read.csv("C:/Users/HP/Downloads/NFLX.csv", header=TRUE)
names(NFLX)
head(NFLX,n=1)
library(tidyr)
NFLX <- read.csv("C:/Users/HP/Downloads/NFLX.csv", header=TRUE)
names(NFLX)
# Calculate the percent missing for each variable in NFLX
NFLX_missingness <- NFLX %>%
summarise_all(~mean(is.na(.)) * 100)
# Print the NFLX_missingness data frame
print(NFLX_missingness)
# Copy NFLX to NFLX1 without assigning data types
NFLX1 <- NFLX
# Drop rows from the data set when a variable has a missing value
NFLX1 <- NFLX1 %>% na.omit()
# Drop rows from the data set where FPI=6
NFLX1 <- NFLX1 %>% filter(FPI != 6)
# Drop ANNTIMS_ACT, ANNTIMS, and REVTIMS
NFLX1 <- NFLX1 %>% select(-ANNTIMS_ACT, -ANNTIMS, -REVTIMS)
# Create a new column named YEAR that is an exact copy of the data in FPEDATS
NFLX1 <- NFLX1 %>% mutate(YEAR = FPEDATS)
# Print out data structure and the summary of NFLX1
str(NFLX1)
summary(NFLX1)
# Calculate the total number of unique analysts providing forecasts each year
NumberAnalyst <- NFLX1 %>%
group_by(YEAR) %>%
distinct(ESTIMATOR) %>%
summarise(NumAnalysts = n_distinct(ESTIMATOR))
# Print the NumberAnalyst object
print(NumberAnalyst)
# Calculate the total number of unique brokerage houses providing forecasts each year
NumberBrokerage <- NFLX1 %>%
group_by(YEAR) %>%
distinct(ESTIMATOR) %>%
summarise(NumBrokerage = n_distinct(ESTIMATOR))
# Print the NumberBrokerage object
print(NumberBrokerage)
# Task 3: Get the most recent forecast in each year
# Get the most recent forecast for each analyst in each year
NFLX2 <- NFLX1 %>%
# Group by analyst and year
group_by(ANALYS, YEAR) %>%
# Filter to keep the most recent forecast for each analyst in each year
filter(
REVDATS == max(REVDATS)
) %>%
# Ungroup the data frame
ungroup()
# Print the dimension of NFLX2
dim(NFLX2)
# Check your work: If your NFLX2 dataset has 641 rows and 14 columns, then you are on the right track. If not, please seek help!
# Create a copy of NFLX2 and call it NFLX3
NFLX3 <- NFLX2
# Task 4: Calculate past accuracy
# For every year within the dataset NFLX3, compute the forecasting performance of each analyst for the current year and store the results in a new column labeled accuracy.
NFLX3 <- NFLX3 %>%
group_by(YEAR, ANALYS) %>%
mutate(accuracy = VALUE - ACTUAL)
# For each year in the NFLX3 dataset, compute the forecasting performance of each analyst from the previous year and store the results in a new column called past_accuracy
NFLX3 <- NFLX3 %>%
group_by(ANALYS) %>%
arrange(YEAR) %>%
mutate(past_accuracy = lag(accuracy))
# Check if the code produces 144 NAs
sum(is.na(NFLX3$past_accuracy))
# Task 5: Forecast Horizon
# Calculate past accuracy and forecast horizon
NFLX3 <- NFLX3 %>% group_by(ANALYS) %>% arrange(YEAR) %>% mutate(past_accuracy = lag(accuracy, default = NA))
NFLX3 <- NFLX3 %>% mutate(horizon = as.numeric(difftime(ANNDATS_ACT, ANNDATS, units = "days")))
# Calculate correlation and find positive correlation year
correlation_by_year <- NFLX3 %>% group_by(YEAR) %>% summarise(correlation = cor(accuracy, horizon, use = "complete.obs"))
positive_corr_year <- correlation_by_year %>% filter(correlation > 0)
# Print positive correlation year with correlation values
positive_corr_year %>% mutate(correlation = round(correlation, 2)) %>% kable() %>% kable_styling(bootstrap_options = "striped")
# Task 6: Experience
# Calculate the cumulative experience for each analyst
NFLX3 <- NFLX3 %>%
group_by(ANALYS) %>%
mutate(experience = cumsum(!duplicated(YEAR)))
# Find the analyst(s) with the highest experience
max_experience <- NFLX3 %>%
group_by(ANALYS) %>%
summarise(experience = max(experience)) %>%
filter(experience == max(experience))
# Print the summary of the experience column
print(summary(NFLX3$experience))
# Find the analyst(s) with the highest experience
max_experience <- NFLX3 %>%
group_by(ANALYS) %>%
summarise(experience = max(experience)) %>%
filter(experience == max(experience))
# Print the summary of the experience column
print(summary(NFLX3$experience))
# Print the analyst(s) with the highest experience
print(max_experience)
# Task 7: Size
# Calculate the total count of unique analysts employed per year by each brokerage house
NFLX3 <- NFLX3 %>%
group_by(YEAR, ESTIMATOR) %>%
mutate(size = n_distinct(ANALYS))
# Print the frequencies for the size variable
size_freq <- table(NFLX3$size)
print(size_freq)
# Create a frequency table for better visualization
size_table <- as.data.frame(size_freq)
colnames(size_table) <- c("Number of Analysts", "Frequency")
# Sort the table by frequency in descending order
size_table <- size_table[order(-size_table$Frequency), ]
# Print the sorted frequency table
print(size_table)
# Summary statistics for size variable
summary(NFLX3$size)
# Calculate the mean of past_accuracy
mean_past_accuracy <- mean(NFLX3$past_accuracy, na.rm = TRUE)
# Convert the 'YEAR' column to the desired format
NFLX3$YEAR <- as.POSIXct(NFLX3$YEAR, format = "%Y-%m-%d %H:%M:%S")
# Create the linear regression model using historical data up to the year 2019
model1 <- lm(ACTUAL ~ VALUE + past_accuracy, data = NFLX3)
# Get the R-squared value of the model
r_squared <- summary(model1)$r.squared
# If the R-squared value is high, then we can use the model to generate a forecast
if (r_squared > 0.5) {
# Create a new data frame for the future period with the values of the independent variables
new_data_future <- data.frame(
VALUE = 6.08, # Replace this with the actual value of VALUE for the future period
past_accuracy = mean_past_accuracy
)
# Predict the EPS for the future period
predicted_eps_future <- predict(model1, newdata = new_data_future)
# Print the forecasted EPS for the future period
cat("Forecasted EPS for future period: $", round(predicted_eps_future, 2))
} else {
# Print a warning message
cat("The R-squared value is low, so the model may not be able to accurately predict future values of the dependent variable.")
}
# Print the mean of past_accuracy
cat("Mean past_accuracy: ", round(mean_past_accuracy, 2))
# Calculate the mean forecast
mean_forecast <- mean(NFLX3$VALUE, na.rm = TRUE)
# Calculate the median forecast
median_forecast <- median(NFLX3$VALUE, na.rm = TRUE)
# Print the mean and median forecasts
cat("Mean forecast for 2020: $", round(mean_forecast, 2))
cat("Median forecast for 2020: $", round(median_forecast, 2))
# Aggregate data and calculate yearly averages, ignoring missing values (NAs)
NFLX4 <- NFLX3 %>%
group_by(YEAR) %>%
summarise(
size = mean(size, na.rm = TRUE),
experience = mean(experience, na.rm = TRUE),
horizon = mean(horizon, na.rm = TRUE),
accuracy = mean(accuracy, na.rm = TRUE),
past_accuracy = mean(past_accuracy, na.rm = TRUE),
ACTUAL = mean(ACTUAL, na.rm = TRUE)
)
# Present a summary of the NFLX4 dataset
summary(NFLX4)
# Correlation analysis
correlation_matrix <- cor(NFLX4[, c("size", "experience", "horizon", "accuracy", "past_accuracy", "ACTUAL")], use = "complete.obs")
# Print correlation matrix
print(correlation_matrix)
# Exploratory data analysis
# Create scatter plots to explore relationships
# Scatter plot of ACTUAL vs. size
ggplot(NFLX4, aes(x = size, y = ACTUAL)) +
geom_point(color = "blue", shape = 1) +
ggtitle("ACTUAL vs. size") +
xlab("size") +
ylab("ACTUAL")
# Scatter plot of ACTUAL vs. experience
ggplot(NFLX4, aes(x = experience, y = ACTUAL)) +
geom_point(color = "green", shape = 2) +
ggtitle("ACTUAL vs. experience") +
xlab("experience") +
ylab("ACTUAL")
# Scatter plot of ACTUAL vs. horizon
ggplot(NFLX4, aes(x = horizon, y = ACTUAL)) +
geom_point(color = "red", shape = 3) +
ggtitle("ACTUAL vs. horizon") +
xlab("horizon") +
ylab("ACTUAL")
# Scatter plot of ACTUAL vs. accuracy
ggplot(NFLX4, aes(x = accuracy, y = ACTUAL)) +
geom_point(color = "orange", shape = 4) +
ggtitle("ACTUAL vs. accuracy") +
xlab("accuracy") +
ylab("ACTUAL")
# Scatter plot of ACTUAL vs. past_accuracy
ggplot(NFLX4, aes(x = past_accuracy, y = ACTUAL)) +
geom_point(color = "purple", shape = 5) +
ggtitle("ACTUAL vs. past_accuracy") +
xlab("past_accuracy") +
ylab("ACTUAL")
# Save NFLX4 to a CSV file if needed
# write.csv(NFLX4, "NFLX4.csv", row.names = FALSE)
# Load necessary libraries
library(ggplot2)
library(dplyr)
library(caret)
library(randomForest)
library(rpart)
library(e1071)
library(kknn)
library(kableExtra)  # Add kableExtra library
# Function to calculate RMSE
calculate_rmse <- function(predictions, actuals) {
sqrt(mean((predictions - actuals)^2))
}
# Generate random data
set.seed(123)
x <- rnorm(100)
y <- 2 * x + rnorm(100)
# Combine data into a data frame
data <- data.frame(x = x, y = y)
# Support Vector Machine (SVM)
svm_model <- svm(y ~ x, data = data)
# k-Nearest Neighbors (k-NN)
knn_model <- kknn(y ~ x, train = data, test = data, k = 3)
# Display the first few rows of the dataset
head(data)
# Summary statistics
summary(data)
# ...
# Model Evaluation
# ...
# Print model evaluation results
cat("Linear Regression Model:\n")
cat("RMSE:", lm_rmse, "\n\n")
# Load necessary libraries
library(ggplot2)
library(dplyr)
library(caret)
library(randomForest)
library(rpart)
library(e1071)
library(kknn)
# Function to calculate RMSE
calculate_rmse <- function(predictions, actuals) {
sqrt(mean((predictions - actuals)^2))
}
# Generate random data
set.seed(123)
x <- rnorm(100)
y <- 2 * x + rnorm(100)
# Combine data into a data frame
data <- data.frame(x = x, y = y)
# Support Vector Machine (SVM)
svm_model <- svm(y ~ x, data = data)
# k-Nearest Neighbors (k-NN)
knn_model <- kknn(y ~ x, train = data, test = data, k = 3)
# Display the first few rows of the dataset
head(data)
# Summary statistics
summary(data)
# Scatter plot of X and Y
ggplot(data, aes(x = x, y = y)) +
geom_point(color = "blue") +
labs(title = "Scatter Plot of X vs Y",
x = "X",
y = "Y") +
theme_minimal()
# Linear Regression
lm_model <- lm(y ~ x, data = data)
# Decision Tree
dt_model <- rpart(y ~ x, data = data)
# Random Forest
rf_model <- randomForest(y ~ x, data = data)
# Model Evaluation
# Make predictions
lm_predictions <- predict(lm_model, newdata = data)
dt_predictions <- predict(dt_model, newdata = data)
rf_predictions <- predict(rf_model, newdata = data)
# Evaluate models
lm_rmse <- calculate_rmse(lm_predictions, data$y)
dt_rmse <- calculate_rmse(dt_predictions, data$y)
rf_rmse <- calculate_rmse(rf_predictions, data$y)
# Print model evaluation results
cat("Linear Regression Model:\n")
cat("RMSE:", lm_rmse, "\n\n")
cat("Decision Tree Model:\n")
cat("RMSE:", dt_rmse, "\n\n")
cat("Random Forest Model:\n")
cat("RMSE:", rf_rmse, "\n\n")
# Plot the data and regression lines for all models
ggplot(data, aes(x = x, y = y)) +
geom_point(color = "blue") +
geom_abline(intercept = coef(lm_model)[1], slope = coef(lm_model)[2], color = "red", linetype = "dashed", size = 1) +
geom_line(data = data.frame(x = sort(data$x), y = predict(dt_model, newdata = data.frame(x = sort(data$x)))), color = "green", size = 1) +
geom_line(data = data.frame(x = sort(data$x), y = predict(rf_model, newdata = data.frame(x = sort(data$x)))), color = "purple", size = 1) +
labs(title = "Linear Regression, Decision Tree, and Random Forest Models",
x = "X",
y = "Y") +
theme_minimal()
# Support Vector Machine (SVM)
svm_model <- svm(y ~ x, data = data)
# k-Nearest Neighbors (k-NN)
knn_model <- kknn(y ~ x, train = data, test = data, k = 3)
# Make predictions for SVM and k-NN
svm_predictions <- predict(svm_model, newdata = data)
# k-Nearest Neighbors (k-NN)
knn_model <- kknn(y ~ x, train = data, test = data, k = 3)
# Extract predicted values from the k-NN model
knn_predictions <- as.vector(knn_model$fitted.values)
# Evaluate k-NN model
knn_rmse <- calculate_rmse(knn_predictions, data$y)
# Print k-NN model evaluation results
cat("k-Nearest Neighbors (k-NN) Model:\n")
cat("RMSE:", knn_rmse, "\n\n")
# Evaluate SVM and k-NN models
svm_rmse <- calculate_rmse(svm_predictions, data$y)
knn_rmse <- calculate_rmse(knn_predictions, data$y)
# Print additional model evaluation results
cat("Support Vector Machine (SVM) Model:\n")
cat("RMSE:", svm_rmse, "\n\n")
cat("k-Nearest Neighbors (k-NN) Model:\n")
cat("RMSE:", knn_rmse, "\n\n")
# Plot the data and regression lines for all models
ggplot(data, aes(x = x, y = y)) +
geom_point(color = "blue") +
geom_abline(intercept = coef(lm_model)[1], slope = coef(lm_model)[2], color = "red", linetype = "dashed", size = 1) +
geom_line(data = data.frame(x = sort(data$x), y = predict(dt_model, newdata = data.frame(x = sort(data$x)))), color = "green", size = 1) +
geom_line(data = data.frame(x = sort(data$x), y = predict(rf_model, newdata = data.frame(x = sort(data$x)))), color = "purple", size = 1) +
geom_line(data = data.frame(x = sort(data$x), y = svm_predictions[order(data$x)]), color = "orange", size = 1) +
geom_line(data = data.frame(x = sort(data$x), y = knn_predictions[order(data$x)]), color = "brown", size = 1) +
labs(title = "Multiple Regression Models Comparison",
x = "X",
y = "Y") +
theme_minimal()
# Load necessary libraries
library(ggplot2)
library(dplyr)
library(caret)
library(randomForest)
library(rpart)
library(e1071)
library(kknn)
library(kableExtra)
# Function to calculate RMSE
calculate_rmse <- function(predictions, actuals) sqrt(mean((predictions - actuals)^2))
# Generate random data
set.seed(123)
data <- data.frame(x = rnorm(100), y = 2 * rnorm(100) + rnorm(100))
# Linear Regression
lm_model <- lm(y ~ x, data = data)
# Decision Tree
dt_model <- rpart(y ~ x, data = data)
# Random Forest
rf_model <- randomForest(y ~ x, data = data)
# Support Vector Machine (SVM)
svm_model <- svm(y ~ x, data = data)
# k-Nearest Neighbors (k-NN)
knn_model <- kknn(y ~ x, train = data, test = data, k = 3)
# Make predictions
lm_predictions <- predict(lm_model, newdata = data)
dt_predictions <- predict(dt_model, newdata = data)
rf_predictions <- predict(rf_model, newdata = data)
svm_predictions <- predict(svm_model, newdata = data)
knn_predictions <- as.vector(knn_model$fitted.values)
# Evaluate models
lm_rmse <- calculate_rmse(lm_predictions, data$y)
dt_rmse <- calculate_rmse(dt_predictions, data$y)
rf_rmse <- calculate_rmse(rf_predictions, data$y)
svm_rmse <- calculate_rmse(svm_predictions, data$y)
knn_rmse <- calculate_rmse(knn_predictions, data$y)
# Model evaluation results
results <- data.frame(
Model = c("Linear Regression", "Decision Tree", "Random Forest", "SVM", "k-NN"),
RMSE = c(lm_rmse, dt_rmse, rf_rmse, svm_rmse, knn_rmse)
)
# Print model evaluation results as a table
results %>%
kable("html") %>%
kable_styling()
# Plot the data and regression lines for all models
ggplot(data, aes(x = x, y = y)) +
geom_point(color = "blue") +
geom_abline(intercept = coef(lm_model)[1], slope = coef(lm_model)[2], color = "red", linetype = "dashed", size = 1) +
geom_line(data = data.frame(x = sort(data$x), y = predict(dt_model, newdata = data.frame(x = sort(data$x)))), color = "green", size = 1) +
geom_line(data = data.frame(x = sort(data$x), y = predict(rf_model, newdata = data.frame(x = sort(data$x)))), color = "purple", size = 1) +
geom_line(data = data.frame(x = sort(data$x), y = svm_predictions[order(data$x)]), color = "orange", size = 1) +
geom_line(data = data.frame(x = sort(data$x), y = knn_predictions[order(data$x)]), color = "brown", size = 1) +
labs(title = "Multiple Regression Models Comparison",
x = "X",
y = "Y") +
theme_minimal()
# Load necessary libraries
library(ggplot2)
library(dplyr)
library(randomForest)
library(rpart)
library(e1071)
library(kknn)
# Generate random data
set.seed(123)
x <- rnorm(100)
y <- 2 * x + rnorm(100)
# Combine data into a data frame
data <- data.frame(x = x, y = y)
# Exploratory Data Analysis (EDA)
# Display the first few rows of the dataset
head(data)
# Summary statistics
summary(data)
# Scatter plot of X and Y
ggplot(data, aes(x = x, y = y)) +
geom_point(color = "blue") +
labs(title = "Scatter Plot of X vs Y",
x = "X",
y = "Y") +
theme_minimal()
# Linear Regression
lm_model <- lm(y ~ x, data = data)
# Decision Tree
dt_model <- rpart(y ~ x, data = data)
# Random Forest
rf_model <- randomForest(y ~ x, data = data)
# Support Vector Machine (SVM)
svm_model <- svm(y ~ x, data = data)
# k-Nearest Neighbors (k-NN)
knn_model <- kknn(y ~ x, train = data, test = data, k = 3)
# Model Evaluation
# Function to calculate RMSE
calculate_rmse <- function(predictions, actuals) {
sqrt(mean((predictions - actuals)^2))
}
# Make predictions
lm_predictions <- predict(lm_model, newdata = data)
dt_predictions <- predict(dt_model, newdata = data)
rf_predictions <- predict(rf_model, newdata = data)
# Evaluate models
lm_rmse <- calculate_rmse(lm_predictions, data$y)
dt_rmse <- calculate_rmse(dt_predictions, data$y)
rf_rmse <- calculate_rmse(rf_predictions, data$y)
# Print model evaluation results
cat("Linear Regression Model:\n")
cat("RMSE:", lm_rmse, "\n\n")
cat("Decision Tree Model:\n")
cat("RMSE:", dt_rmse, "\n\n")
cat("Random Forest Model:\n")
cat("RMSE:", rf_rmse, "\n\n")
# Plot the data and regression lines for all models
ggplot(data, aes(x = x, y = y)) +
geom_point(color = "blue") +
geom_abline(intercept = coef(lm_model)[1], slope = coef(lm_model)[2], color = "red", linetype = "dashed", size = 1) +
geom_line(data = data.frame(x = sort(data$x), y = predict(dt_model, newdata = data.frame(x = sort(data$x)))), color = "green", size = 1) +
geom_line(data = data.frame(x = sort(data$x), y = predict(rf_model, newdata = data.frame(x = sort(data$x)))), color = "purple", size = 1) +
labs(title = "Linear Regression, Decision Tree, and Random Forest Models",
x = "X",
y = "Y") +
theme_minimal()
