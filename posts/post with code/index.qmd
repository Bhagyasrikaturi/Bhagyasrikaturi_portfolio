---
title: "Machine learning model on simple dataframe"
author: "Bhagyasri Katuri"
date: "2023-12-08"
categories: [news, code, analysis,plotly,plot]
image: "b.jpg"
---

```{r}
# Load necessary libraries
library(ggplot2)
library(dplyr)
library(caret)
library(randomForest)
library(rpart)
library(e1071) 
library(kknn) 
library(kableExtra)

# Function to calculate RMSE
calculate_rmse <- function(predictions, actuals) sqrt(mean((predictions - actuals)^2))

# Generate random data
set.seed(123)
data <- data.frame(x = rnorm(100), y = 2 * rnorm(100) + rnorm(100))

# Linear Regression
lm_model <- lm(y ~ x, data = data)

# Decision Tree
dt_model <- rpart(y ~ x, data = data)

# Random Forest
rf_model <- randomForest(y ~ x, data = data)

# Support Vector Machine (SVM)
svm_model <- svm(y ~ x, data = data)

# k-Nearest Neighbors (k-NN)
knn_model <- kknn(y ~ x, train = data, test = data, k = 3)

# Make predictions
lm_predictions <- predict(lm_model, newdata = data)
dt_predictions <- predict(dt_model, newdata = data)
rf_predictions <- predict(rf_model, newdata = data)
svm_predictions <- predict(svm_model, newdata = data)
knn_predictions <- as.vector(knn_model$fitted.values)

# Evaluate models
lm_rmse <- calculate_rmse(lm_predictions, data$y)
dt_rmse <- calculate_rmse(dt_predictions, data$y)
rf_rmse <- calculate_rmse(rf_predictions, data$y)
svm_rmse <- calculate_rmse(svm_predictions, data$y)
knn_rmse <- calculate_rmse(knn_predictions, data$y)

# Model evaluation results
results <- data.frame(
  Model = c("Linear Regression", "Decision Tree", "Random Forest", "SVM", "k-NN"),
  RMSE = c(lm_rmse, dt_rmse, rf_rmse, svm_rmse, knn_rmse)
)

# Print model evaluation results as a table
results %>%
  kable("html") %>%
  kable_styling()

# Plot the data and regression lines for all models
ggplot(data, aes(x = x, y = y)) +
  geom_point(color = "blue") +
  geom_abline(intercept = coef(lm_model)[1], slope = coef(lm_model)[2], color = "red", linetype = "dashed", size = 1) +
  geom_line(data = data.frame(x = sort(data$x), y = predict(dt_model, newdata = data.frame(x = sort(data$x)))), color = "green", size = 1) +
  geom_line(data = data.frame(x = sort(data$x), y = predict(rf_model, newdata = data.frame(x = sort(data$x)))), color = "purple", size = 1) +
  geom_line(data = data.frame(x = sort(data$x), y = svm_predictions[order(data$x)]), color = "orange", size = 1) +
  geom_line(data = data.frame(x = sort(data$x), y = knn_predictions[order(data$x)]), color = "brown", size = 1) +
  labs(title = "Multiple Regression Models Comparison",
       x = "X",
       y = "Y") +
  theme_minimal()
```

***The R code performs a comprehensive comparison of multiple regression models on randomly generated data. The models considered include Linear Regression, Decision Tree, Random Forest, Support Vector Machine (SVM), and k-Nearest Neighbors (k-NN). The code calculates the Root Mean Squared Error (RMSE) for each model to evaluate their predictive performance. The RMSE results are then organized into a table using the kableExtra package and displayed in an HTML-styled table. Additionally, a visualization is created using ggplot2, showcasing the scatter plot of the data along with regression lines for each model. The colors of the lines correspond to the respective models, providing a clear visual representation of their fit to the data. This comprehensive analysis allows for a holistic comparison of the models' performance and aids in identifying the most suitable regression approach for the given data set.***

```{r}
# Load necessary libraries
library(ggplot2)
library(dplyr)
library(randomForest)
library(rpart)
library(e1071) 
library(kknn) 

# Generate random data
set.seed(123)
x <- rnorm(100)
y <- 2 * x + rnorm(100)

# Combine data into a data frame
data <- data.frame(x = x, y = y)

# Exploratory Data Analysis (EDA)

# Display the first few rows of the dataset
head(data)

# Summary statistics
summary(data)

# Scatter plot of X and Y
ggplot(data, aes(x = x, y = y)) +
  geom_point(color = "blue") +
  labs(title = "Scatter Plot of X vs Y",
       x = "X",
       y = "Y") +
  theme_minimal()

# Linear Regression
lm_model <- lm(y ~ x, data = data)

# Decision Tree
dt_model <- rpart(y ~ x, data = data)

# Random Forest
rf_model <- randomForest(y ~ x, data = data)

# Support Vector Machine (SVM)
svm_model <- svm(y ~ x, data = data)

# k-Nearest Neighbors (k-NN)
knn_model <- kknn(y ~ x, train = data, test = data, k = 3)

# Model Evaluation
# Function to calculate RMSE
calculate_rmse <- function(predictions, actuals) {
  sqrt(mean((predictions - actuals)^2))
}

# Make predictions
lm_predictions <- predict(lm_model, newdata = data)
dt_predictions <- predict(dt_model, newdata = data)
rf_predictions <- predict(rf_model, newdata = data)

# Evaluate models
lm_rmse <- calculate_rmse(lm_predictions, data$y)
dt_rmse <- calculate_rmse(dt_predictions, data$y)
rf_rmse <- calculate_rmse(rf_predictions, data$y)

# Print model evaluation results
cat("Linear Regression Model:\n")
cat("RMSE:", lm_rmse, "\n\n")

cat("Decision Tree Model:\n")
cat("RMSE:", dt_rmse, "\n\n")

cat("Random Forest Model:\n")
cat("RMSE:", rf_rmse, "\n\n")

# Plot the data and regression lines for all models
ggplot(data, aes(x = x, y = y)) +
  geom_point(color = "blue") +
  geom_abline(intercept = coef(lm_model)[1], slope = coef(lm_model)[2], color = "red", linetype = "dashed", size = 1) +
  geom_line(data = data.frame(x = sort(data$x), y = predict(dt_model, newdata = data.frame(x = sort(data$x)))), color = "green", size = 1) +
  geom_line(data = data.frame(x = sort(data$x), y = predict(rf_model, newdata = data.frame(x = sort(data$x)))), color = "purple", size = 1) +
  labs(title = "Linear Regression, Decision Tree, and Random Forest Models",
       x = "X",
       y = "Y") +
  theme_minimal()

```

***`The R code conducts an analysis of randomly generated data through various machine learning models. Initially, it generates a data set with a defined relationship between variables X and Y. The exploratory data analysis (EDA) includes displaying the data set's initial rows and presenting summary statistics. Subsequently, it visualizes the relationship between X and Y through a scatter plot. The code then fits several machine learning models, including Linear Regression, Decision Tree, Random Forest, Support Vector Machine (SVM), and k-Nearest Neighbors (k-NN), to the data. Model evaluation is performed by calculating the Root Mean Squared Error (RMSE) for each model, providing a quantitative measure of their predictive accuracy. Finally, the code visually compares the data and regression lines of the Linear Regression, Decision Tree, and Random Forest models in a single plot, offering insights into their respective fits to the underlying data.`***
